{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 01 - Exploratory Data Analysis (EDA)\n",
                "\n",
                "This notebook performs comprehensive EDA on the business events data:\n",
                "- Load Delta/Parquet tables\n",
                "- Distribution plots for numeric features\n",
                "- Correlation matrices\n",
                "- Missingness heatmap\n",
                "- KPI summaries"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import json\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "\n",
                "# Visualization settings\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette('husl')\n",
                "%matplotlib inline\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.float_format', '{:.2f}'.format)\n",
                "\n",
                "print(\"Libraries loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Loading\n",
                "\n",
                "Load from Delta tables when available, with Parquet fallback for development."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_events_data(delta_path: str = None, parquet_path: str = None) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Load events data from Delta Lake or Parquet.\n",
                "    Falls back to Parquet if Delta is unavailable.\n",
                "    \"\"\"\n",
                "    # Try Delta first\n",
                "    if delta_path:\n",
                "        try:\n",
                "            from deltalake import DeltaTable\n",
                "            dt = DeltaTable(delta_path)\n",
                "            df = dt.to_pandas()\n",
                "            print(f\"Loaded {len(df):,} rows from Delta table: {delta_path}\")\n",
                "            return df\n",
                "        except Exception as e:\n",
                "            print(f\"Delta loading failed: {e}\")\n",
                "    \n",
                "    # Fallback to Parquet\n",
                "    if parquet_path:\n",
                "        df = pd.read_parquet(parquet_path)\n",
                "        print(f\"Loaded {len(df):,} rows from Parquet: {parquet_path}\")\n",
                "        return df\n",
                "    \n",
                "    raise FileNotFoundError(\"No valid data source provided\")\n",
                "\n",
                "\n",
                "# Configure paths\n",
                "PROJECT_ROOT = Path(\".\").resolve().parent\n",
                "DELTA_PATH = PROJECT_ROOT / \"data\" / \"lake\" / \"delta\" / \"events\"  # Future Delta location\n",
                "PARQUET_PATH = PROJECT_ROOT / \"data\" / \"sample\" / \"events.parquet\"\n",
                "\n",
                "# Load data\n",
                "df = load_events_data(\n",
                "    delta_path=str(DELTA_PATH) if DELTA_PATH.exists() else None,\n",
                "    parquet_path=str(PARQUET_PATH)\n",
                ")\n",
                "\n",
                "print(f\"\\nDataset shape: {df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Parse metadata JSON if stored as string\n",
                "if 'metadata' in df.columns and df['metadata'].dtype == 'object':\n",
                "    df['metadata'] = df['metadata'].apply(\n",
                "        lambda x: json.loads(x) if isinstance(x, str) else x\n",
                "    )\n",
                "\n",
                "# Extract metadata fields\n",
                "df['channel'] = df['metadata'].apply(lambda x: x.get('channel') if isinstance(x, dict) else None)\n",
                "df['device_type'] = df['metadata'].apply(lambda x: x.get('device_type') if isinstance(x, dict) else None)\n",
                "\n",
                "# Parse timestamp\n",
                "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
                "df['date'] = df['timestamp'].dt.date\n",
                "df['hour'] = df['timestamp'].dt.hour\n",
                "df['day_of_week'] = df['timestamp'].dt.day_name()\n",
                "\n",
                "# Calculate revenue\n",
                "df['revenue'] = df['price'] * df['quantity']\n",
                "\n",
                "print(\"Feature engineering complete!\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Overview"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"DATASET OVERVIEW\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nShape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
                "print(f\"\\nDate range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
                "print(f\"Time span: {(df['timestamp'].max() - df['timestamp'].min()).days} days\")\n",
                "print(\"\\nColumn types:\")\n",
                "print(df.dtypes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Distribution Plots\n",
                "\n",
                "Visualize the distribution of key numeric features to understand data characteristics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "fig.suptitle('Distribution of Numeric Features', fontsize=16, fontweight='bold')\n",
                "\n",
                "# Price distribution\n",
                "ax1 = axes[0, 0]\n",
                "sns.histplot(df['price'], bins=50, kde=True, ax=ax1, color='steelblue')\n",
                "ax1.set_title('Price Distribution')\n",
                "ax1.set_xlabel('Price ($)')\n",
                "ax1.axvline(df['price'].mean(), color='red', linestyle='--', label=f'Mean: ${df[\"price\"].mean():.2f}')\n",
                "ax1.axvline(df['price'].median(), color='orange', linestyle='--', label=f'Median: ${df[\"price\"].median():.2f}')\n",
                "ax1.legend()\n",
                "\n",
                "# Quantity distribution\n",
                "ax2 = axes[0, 1]\n",
                "sns.histplot(df['quantity'], bins=10, kde=False, ax=ax2, color='coral')\n",
                "ax2.set_title('Quantity Distribution')\n",
                "ax2.set_xlabel('Quantity')\n",
                "\n",
                "# Revenue distribution (log scale)\n",
                "ax3 = axes[1, 0]\n",
                "sns.histplot(df['revenue'], bins=50, kde=True, ax=ax3, color='seagreen')\n",
                "ax3.set_title('Revenue Distribution')\n",
                "ax3.set_xlabel('Revenue ($)')\n",
                "ax3.axvline(df['revenue'].mean(), color='red', linestyle='--', label=f'Mean: ${df[\"revenue\"].mean():.2f}')\n",
                "ax3.legend()\n",
                "\n",
                "# Hourly distribution\n",
                "ax4 = axes[1, 1]\n",
                "hourly_counts = df['hour'].value_counts().sort_index()\n",
                "ax4.bar(hourly_counts.index, hourly_counts.values, color='mediumpurple')\n",
                "ax4.set_title('Events by Hour of Day')\n",
                "ax4.set_xlabel('Hour')\n",
                "ax4.set_ylabel('Event Count')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('eda_distributions.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Interpretation:**\n",
                "- **Price**: Shows the price range distribution. A right-skewed distribution indicates more low-priced transactions.\n",
                "- **Quantity**: Integer distribution from 1-10, showing purchase behavior patterns.\n",
                "- **Revenue**: Combination of price × quantity, revealing the business value distribution.\n",
                "- **Hourly Pattern**: Identifies peak transaction hours for operational insights."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Correlation Matrix\n",
                "\n",
                "Analyze relationships between numeric features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select numeric columns for correlation\n",
                "numeric_cols = ['price', 'quantity', 'revenue', 'hour']\n",
                "corr_matrix = df[numeric_cols].corr()\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
                "\n",
                "sns.heatmap(\n",
                "    corr_matrix,\n",
                "    mask=mask,\n",
                "    annot=True,\n",
                "    fmt='.3f',\n",
                "    cmap='RdBu_r',\n",
                "    center=0,\n",
                "    square=True,\n",
                "    linewidths=0.5,\n",
                "    ax=ax,\n",
                "    cbar_kws={'shrink': 0.8}\n",
                ")\n",
                "\n",
                "ax.set_title('Correlation Matrix - Numeric Features', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('eda_correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nCorrelation values:\")\n",
                "print(corr_matrix)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Interpretation:**\n",
                "- Revenue is strongly correlated with both price and quantity (expected, as revenue = price × quantity)\n",
                "- Price and quantity may show weak or no correlation, indicating independent purchasing decisions\n",
                "- Hour shows minimal correlation with transaction values, suggesting consistent behavior across the day"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Missingness Heatmap\n",
                "\n",
                "Identify patterns in missing data across the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate missingness\n",
                "missing_df = df.isnull()\n",
                "missing_pct = (missing_df.sum() / len(df) * 100).sort_values(ascending=False)\n",
                "\n",
                "print(\"Missing Value Summary:\")\n",
                "print(\"=\" * 40)\n",
                "for col, pct in missing_pct.items():\n",
                "    print(f\"{col:<20} {pct:>6.2f}%\")\n",
                "\n",
                "# Missingness heatmap (sample for large datasets)\n",
                "sample_size = min(1000, len(df))\n",
                "sample_idx = np.random.choice(len(df), sample_size, replace=False)\n",
                "sample_idx = np.sort(sample_idx)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(14, 6))\n",
                "sns.heatmap(\n",
                "    df.iloc[sample_idx].isnull().T,\n",
                "    cbar=True,\n",
                "    cmap='YlOrRd',\n",
                "    ax=ax,\n",
                "    yticklabels=True\n",
                ")\n",
                "ax.set_title(f'Missingness Heatmap (Sample of {sample_size:,} rows)', fontsize=14, fontweight='bold')\n",
                "ax.set_xlabel('Row Index (sampled)')\n",
                "ax.set_ylabel('Column')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('eda_missingness_heatmap.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Interpretation:**\n",
                "- Yellow/red cells indicate missing values\n",
                "- Vertical patterns suggest systematic missing data in specific columns\n",
                "- Block patterns may indicate batch ingestion issues\n",
                "- Fully dark rows = complete records"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. KPI Summaries\n",
                "\n",
                "Calculate key business metrics and performance indicators."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate KPIs\n",
                "kpis = {\n",
                "    'Total Transactions': f\"{len(df):,}\",\n",
                "    'Total Revenue': f\"${df['revenue'].sum():,.2f}\",\n",
                "    'Average Order Value': f\"${df['revenue'].mean():.2f}\",\n",
                "    'Median Order Value': f\"${df['revenue'].median():.2f}\",\n",
                "    'Unique Users': f\"{df['user_id'].nunique():,}\",\n",
                "    'Unique Products': f\"{df['product_id'].nunique():,}\",\n",
                "    'Unique Locations': f\"{df['location'].nunique():,}\",\n",
                "    'Avg Items per Transaction': f\"{df['quantity'].mean():.2f}\",\n",
                "    'Max Single Transaction': f\"${df['revenue'].max():,.2f}\",\n",
                "    'Date Range': f\"{df['date'].min()} to {df['date'].max()}\",\n",
                "}\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"KEY PERFORMANCE INDICATORS (KPIs)\")\n",
                "print(\"=\" * 60)\n",
                "for kpi, value in kpis.items():\n",
                "    print(f\"{kpi:<30} {value}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Channel breakdown\n",
                "print(\"\\nChannel Distribution:\")\n",
                "print(\"=\" * 40)\n",
                "channel_stats = df.groupby('channel').agg({\n",
                "    'id': 'count',\n",
                "    'revenue': ['sum', 'mean']\n",
                "}).round(2)\n",
                "channel_stats.columns = ['Transactions', 'Total Revenue', 'Avg Revenue']\n",
                "channel_stats['% of Transactions'] = (channel_stats['Transactions'] / len(df) * 100).round(2)\n",
                "print(channel_stats.sort_values('Transactions', ascending=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize channel distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Transaction count by channel\n",
                "ax1 = axes[0]\n",
                "channel_counts = df['channel'].value_counts()\n",
                "colors = sns.color_palette('husl', len(channel_counts))\n",
                "ax1.pie(channel_counts, labels=channel_counts.index, autopct='%1.1f%%', colors=colors, startangle=90)\n",
                "ax1.set_title('Transactions by Channel', fontsize=12, fontweight='bold')\n",
                "\n",
                "# Revenue by channel\n",
                "ax2 = axes[1]\n",
                "revenue_by_channel = df.groupby('channel')['revenue'].sum().sort_values(ascending=True)\n",
                "ax2.barh(revenue_by_channel.index, revenue_by_channel.values, color=colors)\n",
                "ax2.set_title('Total Revenue by Channel', fontsize=12, fontweight='bold')\n",
                "ax2.set_xlabel('Revenue ($)')\n",
                "for i, v in enumerate(revenue_by_channel.values):\n",
                "    ax2.text(v + revenue_by_channel.max()*0.01, i, f'${v:,.0f}', va='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('eda_channel_distribution.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Daily Trends"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Daily aggregations\n",
                "daily_stats = df.groupby('date').agg({\n",
                "    'id': 'count',\n",
                "    'revenue': 'sum',\n",
                "    'user_id': 'nunique'\n",
                "}).reset_index()\n",
                "daily_stats.columns = ['date', 'transactions', 'revenue', 'unique_users']\n",
                "\n",
                "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
                "fig.suptitle('Daily Trends', fontsize=16, fontweight='bold')\n",
                "\n",
                "# Transactions\n",
                "axes[0].plot(daily_stats['date'], daily_stats['transactions'], color='steelblue', linewidth=2)\n",
                "axes[0].fill_between(daily_stats['date'], daily_stats['transactions'], alpha=0.3)\n",
                "axes[0].set_ylabel('Transactions')\n",
                "axes[0].set_title('Daily Transactions')\n",
                "\n",
                "# Revenue\n",
                "axes[1].plot(daily_stats['date'], daily_stats['revenue'], color='seagreen', linewidth=2)\n",
                "axes[1].fill_between(daily_stats['date'], daily_stats['revenue'], alpha=0.3, color='seagreen')\n",
                "axes[1].set_ylabel('Revenue ($)')\n",
                "axes[1].set_title('Daily Revenue')\n",
                "\n",
                "# Unique users\n",
                "axes[2].plot(daily_stats['date'], daily_stats['unique_users'], color='coral', linewidth=2)\n",
                "axes[2].fill_between(daily_stats['date'], daily_stats['unique_users'], alpha=0.3, color='coral')\n",
                "axes[2].set_ylabel('Unique Users')\n",
                "axes[2].set_title('Daily Active Users')\n",
                "axes[2].set_xlabel('Date')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('eda_daily_trends.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Interpretation:**\n",
                "- Look for weekly seasonality (weekday vs weekend patterns)\n",
                "- Identify any anomalous spikes or dips that may need investigation\n",
                "- User count trends indicate engagement/acquisition patterns"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Summary & Next Steps\n",
                "\n",
                "### Key Findings\n",
                "1. **Data Quality**: Identified any missing values and data completeness\n",
                "2. **Distribution Characteristics**: Understood price, quantity, and revenue distributions\n",
                "3. **Feature Relationships**: Analyzed correlations between numeric features\n",
                "4. **Business Metrics**: Established baseline KPIs for monitoring\n",
                "\n",
                "### Recommended Next Steps\n",
                "- Proceed to baseline modeling (`02_baselines.ipynb`)\n",
                "- Engineer additional features based on user behavior patterns\n",
                "- Set up automated data quality checks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\nEDA Complete! Notebook executed at: {datetime.now().isoformat()}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
