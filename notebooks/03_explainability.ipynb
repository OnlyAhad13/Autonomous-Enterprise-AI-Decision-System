{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03 - Model Explainability & Fairness Analysis\n",
                "\n",
                "This notebook provides comprehensive model explainability:\n",
                "- SHAP values computation for tree-based models\n",
                "- Summary and force plots\n",
                "- Per-segment performance analysis\n",
                "- Automated fairness checks (demographic parity, equalized odds)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import json\n",
                "from pathlib import Path\n",
                "from datetime import datetime, timedelta\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# ML imports\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    roc_auc_score, confusion_matrix\n",
                ")\n",
                "import xgboost as xgb\n",
                "import lightgbm as lgb\n",
                "\n",
                "# SHAP for explainability\n",
                "import shap\n",
                "\n",
                "# MLflow\n",
                "import mlflow\n",
                "\n",
                "# Visualization settings\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "%matplotlib inline\n",
                "\n",
                "# Initialize SHAP JS visualization\n",
                "shap.initjs()\n",
                "\n",
                "print(\"Libraries loaded successfully!\")\n",
                "print(f\"SHAP version: {shap.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Project paths\n",
                "PROJECT_ROOT = Path(\".\").resolve().parent\n",
                "DATA_PATH = PROJECT_ROOT / \"data\" / \"sample\" / \"events.parquet\"\n",
                "OUTPUT_DIR = Path(\".\")\n",
                "\n",
                "# MLflow configuration\n",
                "MLFLOW_TRACKING_URI = PROJECT_ROOT / \"mlruns\"\n",
                "mlflow.set_tracking_uri(str(MLFLOW_TRACKING_URI))\n",
                "\n",
                "print(f\"Project root: {PROJECT_ROOT}\")\n",
                "print(f\"Data path: {DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading & Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_and_prepare_data(data_path: Path) -> pd.DataFrame:\n",
                "    \"\"\"Load and prepare events data.\"\"\"\n",
                "    df = pd.read_parquet(data_path)\n",
                "    print(f\"Loaded {len(df):,} events\")\n",
                "    \n",
                "    # Parse metadata\n",
                "    if 'metadata' in df.columns and df['metadata'].dtype == 'object':\n",
                "        df['metadata'] = df['metadata'].apply(\n",
                "            lambda x: json.loads(x) if isinstance(x, str) else x\n",
                "        )\n",
                "    \n",
                "    # Extract features\n",
                "    df['channel'] = df['metadata'].apply(lambda x: x.get('channel') if isinstance(x, dict) else 'unknown')\n",
                "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
                "    df['revenue'] = df['price'] * df['quantity']\n",
                "    \n",
                "    # Extract region from location\n",
                "    if 'location' in df.columns:\n",
                "        df['region'] = df['location'].apply(\n",
                "            lambda x: x.split(',')[-1].strip() if isinstance(x, str) and ',' in x else 'Unknown'\n",
                "        )\n",
                "    else:\n",
                "        df['region'] = 'Unknown'\n",
                "    \n",
                "    return df\n",
                "\n",
                "\n",
                "df = load_and_prepare_data(DATA_PATH)\n",
                "print(f\"\\nDataset shape: {df.shape}\")\n",
                "print(f\"Unique regions: {df['region'].nunique()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_user_features(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"Create user-level features with cohort segmentation.\"\"\"\n",
                "    max_date = df['timestamp'].max()\n",
                "    \n",
                "    user_features = df.groupby('user_id').agg({\n",
                "        'id': 'count',\n",
                "        'revenue': ['sum', 'mean', 'std'],\n",
                "        'price': ['mean', 'max', 'min'],\n",
                "        'quantity': ['sum', 'mean'],\n",
                "        'timestamp': ['min', 'max'],\n",
                "        'channel': lambda x: x.mode().iloc[0] if len(x) > 0 else 'unknown',\n",
                "        'region': lambda x: x.mode().iloc[0] if len(x) > 0 else 'Unknown'\n",
                "    }).reset_index()\n",
                "    \n",
                "    # Flatten column names\n",
                "    user_features.columns = [\n",
                "        'user_id', 'transaction_count', 'total_revenue', 'avg_revenue', 'std_revenue',\n",
                "        'avg_price', 'max_price', 'min_price', 'total_quantity', 'avg_quantity',\n",
                "        'first_transaction', 'last_transaction', 'primary_channel', 'primary_region'\n",
                "    ]\n",
                "    \n",
                "    # Derived features\n",
                "    user_features['days_since_first'] = (max_date - user_features['first_transaction']).dt.days\n",
                "    user_features['days_since_last'] = (max_date - user_features['last_transaction']).dt.days\n",
                "    user_features['avg_days_between'] = user_features['days_since_first'] / user_features['transaction_count'].clip(lower=1)\n",
                "    user_features['std_revenue'] = user_features['std_revenue'].fillna(0)\n",
                "    \n",
                "    # Create user cohorts based on total revenue\n",
                "    user_features['user_cohort'] = pd.qcut(\n",
                "        user_features['total_revenue'],\n",
                "        q=3,\n",
                "        labels=['Low Value', 'Medium Value', 'High Value']\n",
                "    )\n",
                "    \n",
                "    # Create churn label (synthetic based on recency and revenue)\n",
                "    revenue_threshold = user_features['total_revenue'].quantile(0.35)\n",
                "    recency_threshold = user_features['days_since_last'].quantile(0.65)\n",
                "    \n",
                "    user_features['churned'] = (\n",
                "        (user_features['total_revenue'] < revenue_threshold) |\n",
                "        ((user_features['total_revenue'] < user_features['total_revenue'].quantile(0.5)) &\n",
                "         (user_features['days_since_last'] > recency_threshold))\n",
                "    ).astype(int)\n",
                "    \n",
                "    # Drop timestamp columns\n",
                "    user_features = user_features.drop(['first_transaction', 'last_transaction'], axis=1)\n",
                "    \n",
                "    return user_features\n",
                "\n",
                "\n",
                "user_df = create_user_features(df)\n",
                "print(f\"User dataset: {user_df.shape}\")\n",
                "print(f\"\\nChurn rate: {user_df['churned'].mean():.2%}\")\n",
                "print(f\"\\nUser cohort distribution:\")\n",
                "print(user_df['user_cohort'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train Tree Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define features\n",
                "feature_cols = [\n",
                "    'transaction_count', 'total_revenue', 'avg_revenue', 'std_revenue',\n",
                "    'avg_price', 'max_price', 'min_price', 'total_quantity', 'avg_quantity',\n",
                "    'days_since_first', 'days_since_last', 'avg_days_between'\n",
                "]\n",
                "\n",
                "X = user_df[feature_cols]\n",
                "y = user_df['churned']\n",
                "\n",
                "# Keep metadata for segment analysis\n",
                "segment_info = user_df[['user_id', 'primary_region', 'user_cohort', 'primary_channel']].copy()\n",
                "\n",
                "# Train/test split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# Split segment info accordingly\n",
                "segment_test = segment_info.iloc[X_test.index].reset_index(drop=True)\n",
                "X_test_reset = X_test.reset_index(drop=True)\n",
                "y_test_reset = y_test.reset_index(drop=True)\n",
                "\n",
                "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
                "print(f\"Test set: {X_test.shape[0]:,} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train XGBoost model\n",
                "xgb_model = xgb.XGBClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=5,\n",
                "    learning_rate=0.1,\n",
                "    random_state=42,\n",
                "    eval_metric='logloss'\n",
                ")\n",
                "xgb_model.fit(X_train, y_train)\n",
                "\n",
                "# Train LightGBM model\n",
                "lgb_model = lgb.LGBMClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=5,\n",
                "    learning_rate=0.1,\n",
                "    random_state=42,\n",
                "    verbose=-1\n",
                ")\n",
                "lgb_model.fit(X_train, y_train)\n",
                "\n",
                "# Get predictions\n",
                "xgb_pred = xgb_model.predict(X_test_reset)\n",
                "lgb_pred = lgb_model.predict(X_test_reset)\n",
                "\n",
                "print(\"XGBoost Metrics:\")\n",
                "print(f\"  Accuracy: {accuracy_score(y_test_reset, xgb_pred):.4f}\")\n",
                "print(f\"  AUC-ROC: {roc_auc_score(y_test_reset, xgb_model.predict_proba(X_test_reset)[:, 1]):.4f}\")\n",
                "\n",
                "print(\"\\nLightGBM Metrics:\")\n",
                "print(f\"  Accuracy: {accuracy_score(y_test_reset, lgb_pred):.4f}\")\n",
                "print(f\"  AUC-ROC: {roc_auc_score(y_test_reset, lgb_model.predict_proba(X_test_reset)[:, 1]):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. SHAP Values Computation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute SHAP values for XGBoost\n",
                "print(\"Computing SHAP values for XGBoost...\")\n",
                "xgb_explainer = shap.TreeExplainer(xgb_model)\n",
                "xgb_shap_values = xgb_explainer.shap_values(X_test_reset)\n",
                "\n",
                "print(\"Computing SHAP values for LightGBM...\")\n",
                "lgb_explainer = shap.TreeExplainer(lgb_model)\n",
                "lgb_shap_values = lgb_explainer.shap_values(X_test_reset)\n",
                "\n",
                "# For binary classification, LightGBM returns list of [class_0, class_1]\n",
                "if isinstance(lgb_shap_values, list):\n",
                "    lgb_shap_values = lgb_shap_values[1]  # Take positive class\n",
                "\n",
                "print(f\"\\nXGBoost SHAP values shape: {xgb_shap_values.shape}\")\n",
                "print(f\"LightGBM SHAP values shape: {lgb_shap_values.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. SHAP Summary Plots"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# XGBoost Summary Plot (Beeswarm)\n",
                "print(\"XGBoost SHAP Summary Plot\")\n",
                "plt.figure(figsize=(12, 8))\n",
                "shap.summary_plot(xgb_shap_values, X_test_reset, feature_names=feature_cols, show=False)\n",
                "plt.title('XGBoost - SHAP Feature Importance', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('shap_summary_xgboost.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LightGBM Summary Plot (Beeswarm)\n",
                "print(\"LightGBM SHAP Summary Plot\")\n",
                "plt.figure(figsize=(12, 8))\n",
                "shap.summary_plot(lgb_shap_values, X_test_reset, feature_names=feature_cols, show=False)\n",
                "plt.title('LightGBM - SHAP Feature Importance', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('shap_summary_lightgbm.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Bar plot of mean absolute SHAP values\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# XGBoost\n",
                "shap.summary_plot(xgb_shap_values, X_test_reset, feature_names=feature_cols, \n",
                "                  plot_type='bar', show=False, ax=axes[0])\n",
                "axes[0].set_title('XGBoost - Mean |SHAP|', fontsize=12, fontweight='bold')\n",
                "\n",
                "# LightGBM\n",
                "shap.summary_plot(lgb_shap_values, X_test_reset, feature_names=feature_cols,\n",
                "                  plot_type='bar', show=False, ax=axes[1])\n",
                "axes[1].set_title('LightGBM - Mean |SHAP|', fontsize=12, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('shap_bar_comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. SHAP Force Plots"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Force plot for a single prediction (XGBoost)\n",
                "sample_idx = 0\n",
                "print(f\"Force plot for sample {sample_idx}:\")\n",
                "print(f\"  Actual: {'Churned' if y_test_reset.iloc[sample_idx] == 1 else 'Not Churned'}\")\n",
                "print(f\"  Predicted: {'Churned' if xgb_pred[sample_idx] == 1 else 'Not Churned'}\")\n",
                "print(f\"  Probability: {xgb_model.predict_proba(X_test_reset)[sample_idx, 1]:.3f}\")\n",
                "\n",
                "shap.force_plot(\n",
                "    xgb_explainer.expected_value,\n",
                "    xgb_shap_values[sample_idx],\n",
                "    X_test_reset.iloc[sample_idx],\n",
                "    feature_names=feature_cols,\n",
                "    matplotlib=True,\n",
                "    show=False\n",
                ")\n",
                "plt.title('XGBoost Force Plot - Single Prediction', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.savefig('shap_force_plot_single.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Force plot for multiple predictions (interactive in notebook)\n",
                "print(\"Interactive force plot for first 100 predictions:\")\n",
                "shap.force_plot(\n",
                "    xgb_explainer.expected_value,\n",
                "    xgb_shap_values[:100],\n",
                "    X_test_reset.iloc[:100],\n",
                "    feature_names=feature_cols\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dependence plot for top feature\n",
                "top_feature = 'total_revenue'\n",
                "print(f\"\\nDependence plot for '{top_feature}':\")\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "shap.dependence_plot(\n",
                "    top_feature,\n",
                "    xgb_shap_values,\n",
                "    X_test_reset,\n",
                "    feature_names=feature_cols,\n",
                "    show=False\n",
                ")\n",
                "plt.title(f'SHAP Dependence Plot - {top_feature}', fontsize=12, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('shap_dependence_total_revenue.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Per-Segment Performance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_segment_metrics(y_true, y_pred, y_prob, segment_labels):\n",
                "    \"\"\"Compute metrics per segment.\"\"\"\n",
                "    results = []\n",
                "    \n",
                "    for segment in segment_labels.unique():\n",
                "        mask = segment_labels == segment\n",
                "        n_samples = mask.sum()\n",
                "        \n",
                "        if n_samples < 5:  # Skip segments with too few samples\n",
                "            continue\n",
                "        \n",
                "        segment_y_true = y_true[mask]\n",
                "        segment_y_pred = y_pred[mask]\n",
                "        segment_y_prob = y_prob[mask]\n",
                "        \n",
                "        # Handle edge case where segment has only one class\n",
                "        if len(segment_y_true.unique()) < 2:\n",
                "            auc = np.nan\n",
                "        else:\n",
                "            auc = roc_auc_score(segment_y_true, segment_y_prob)\n",
                "        \n",
                "        results.append({\n",
                "            'segment': segment,\n",
                "            'n_samples': n_samples,\n",
                "            'churn_rate': segment_y_true.mean(),\n",
                "            'accuracy': accuracy_score(segment_y_true, segment_y_pred),\n",
                "            'precision': precision_score(segment_y_true, segment_y_pred, zero_division=0),\n",
                "            'recall': recall_score(segment_y_true, segment_y_pred, zero_division=0),\n",
                "            'f1': f1_score(segment_y_true, segment_y_pred, zero_division=0),\n",
                "            'auc_roc': auc\n",
                "        })\n",
                "    \n",
                "    return pd.DataFrame(results)\n",
                "\n",
                "\n",
                "# Get predictions and probabilities\n",
                "xgb_prob = xgb_model.predict_proba(X_test_reset)[:, 1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Performance by Region\n",
                "print(\"=\" * 60)\n",
                "print(\"PERFORMANCE BY REGION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "region_metrics = compute_segment_metrics(\n",
                "    y_test_reset,\n",
                "    xgb_pred,\n",
                "    xgb_prob,\n",
                "    segment_test['primary_region']\n",
                ")\n",
                "\n",
                "print(region_metrics.to_string(index=False))\n",
                "\n",
                "# Visualize\n",
                "if len(region_metrics) > 0:\n",
                "    fig, ax = plt.subplots(figsize=(12, 6))\n",
                "    x = np.arange(len(region_metrics))\n",
                "    width = 0.2\n",
                "    \n",
                "    ax.bar(x - width*1.5, region_metrics['accuracy'], width, label='Accuracy', color='steelblue')\n",
                "    ax.bar(x - width/2, region_metrics['precision'], width, label='Precision', color='coral')\n",
                "    ax.bar(x + width/2, region_metrics['recall'], width, label='Recall', color='seagreen')\n",
                "    ax.bar(x + width*1.5, region_metrics['f1'], width, label='F1', color='gold')\n",
                "    \n",
                "    ax.set_xticks(x)\n",
                "    ax.set_xticklabels(region_metrics['segment'], rotation=45, ha='right')\n",
                "    ax.set_ylabel('Score')\n",
                "    ax.set_title('Model Performance by Region', fontsize=12, fontweight='bold')\n",
                "    ax.legend()\n",
                "    ax.set_ylim(0, 1.1)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('segment_performance_region.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Performance by User Cohort\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"PERFORMANCE BY USER COHORT\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "cohort_metrics = compute_segment_metrics(\n",
                "    y_test_reset,\n",
                "    xgb_pred,\n",
                "    xgb_prob,\n",
                "    segment_test['user_cohort'].astype(str)\n",
                ")\n",
                "\n",
                "print(cohort_metrics.to_string(index=False))\n",
                "\n",
                "# Visualize\n",
                "if len(cohort_metrics) > 0:\n",
                "    fig, ax = plt.subplots(figsize=(10, 6))\n",
                "    x = np.arange(len(cohort_metrics))\n",
                "    width = 0.2\n",
                "    \n",
                "    ax.bar(x - width*1.5, cohort_metrics['accuracy'], width, label='Accuracy', color='steelblue')\n",
                "    ax.bar(x - width/2, cohort_metrics['precision'], width, label='Precision', color='coral')\n",
                "    ax.bar(x + width/2, cohort_metrics['recall'], width, label='Recall', color='seagreen')\n",
                "    ax.bar(x + width*1.5, cohort_metrics['f1'], width, label='F1', color='gold')\n",
                "    \n",
                "    ax.set_xticks(x)\n",
                "    ax.set_xticklabels(cohort_metrics['segment'])\n",
                "    ax.set_ylabel('Score')\n",
                "    ax.set_title('Model Performance by User Cohort', fontsize=12, fontweight='bold')\n",
                "    ax.legend()\n",
                "    ax.set_ylim(0, 1.1)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('segment_performance_cohort.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Performance by Channel\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"PERFORMANCE BY CHANNEL\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "channel_metrics = compute_segment_metrics(\n",
                "    y_test_reset,\n",
                "    xgb_pred,\n",
                "    xgb_prob,\n",
                "    segment_test['primary_channel']\n",
                ")\n",
                "\n",
                "print(channel_metrics.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Automated Fairness Checks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_demographic_parity(y_pred, sensitive_feature):\n",
                "    \"\"\"\n",
                "    Compute demographic parity (statistical parity).\n",
                "    \n",
                "    Demographic parity is satisfied when the selection rate (positive prediction rate)\n",
                "    is equal across all groups defined by the sensitive feature.\n",
                "    \n",
                "    Returns: DataFrame with selection rates per group and parity ratio.\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    \n",
                "    for group in sensitive_feature.unique():\n",
                "        mask = sensitive_feature == group\n",
                "        n_samples = mask.sum()\n",
                "        selection_rate = y_pred[mask].mean()\n",
                "        \n",
                "        results.append({\n",
                "            'group': group,\n",
                "            'n_samples': n_samples,\n",
                "            'selection_rate': selection_rate\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(results)\n",
                "    \n",
                "    # Compute parity ratio (min/max selection rate)\n",
                "    min_rate = df['selection_rate'].min()\n",
                "    max_rate = df['selection_rate'].max()\n",
                "    parity_ratio = min_rate / max_rate if max_rate > 0 else 1.0\n",
                "    \n",
                "    return df, parity_ratio\n",
                "\n",
                "\n",
                "def compute_equalized_odds(y_true, y_pred, sensitive_feature):\n",
                "    \"\"\"\n",
                "    Compute equalized odds metrics.\n",
                "    \n",
                "    Equalized odds is satisfied when both TPR and FPR are equal across groups.\n",
                "    \n",
                "    Returns: DataFrame with TPR and FPR per group.\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    \n",
                "    for group in sensitive_feature.unique():\n",
                "        mask = sensitive_feature == group\n",
                "        group_y_true = y_true[mask]\n",
                "        group_y_pred = y_pred[mask]\n",
                "        \n",
                "        # True Positive Rate (TPR) = Recall\n",
                "        positives = group_y_true == 1\n",
                "        if positives.sum() > 0:\n",
                "            tpr = (group_y_pred[positives] == 1).mean()\n",
                "        else:\n",
                "            tpr = np.nan\n",
                "        \n",
                "        # False Positive Rate (FPR)\n",
                "        negatives = group_y_true == 0\n",
                "        if negatives.sum() > 0:\n",
                "            fpr = (group_y_pred[negatives] == 1).mean()\n",
                "        else:\n",
                "            fpr = np.nan\n",
                "        \n",
                "        results.append({\n",
                "            'group': group,\n",
                "            'n_samples': mask.sum(),\n",
                "            'tpr': tpr,\n",
                "            'fpr': fpr\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(results)\n",
                "    \n",
                "    # Compute equalized odds gap (max difference in TPR and FPR)\n",
                "    tpr_gap = df['tpr'].max() - df['tpr'].min()\n",
                "    fpr_gap = df['fpr'].max() - df['fpr'].min()\n",
                "    \n",
                "    return df, tpr_gap, fpr_gap"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fairness Analysis by User Cohort\n",
                "print(\"=\" * 60)\n",
                "print(\"FAIRNESS ANALYSIS - BY USER COHORT\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Demographic Parity\n",
                "dp_df, dp_ratio = compute_demographic_parity(\n",
                "    xgb_pred,\n",
                "    segment_test['user_cohort'].astype(str)\n",
                ")\n",
                "\n",
                "print(\"\\n--- Demographic Parity ---\")\n",
                "print(dp_df.to_string(index=False))\n",
                "print(f\"\\nParity Ratio (min/max selection rate): {dp_ratio:.3f}\")\n",
                "print(f\"{'✓ PASS' if dp_ratio >= 0.8 else '✗ FAIL'}: Threshold is 0.80 (80% rule)\")\n",
                "\n",
                "# Equalized Odds\n",
                "eo_df, tpr_gap, fpr_gap = compute_equalized_odds(\n",
                "    y_test_reset.values,\n",
                "    xgb_pred,\n",
                "    segment_test['user_cohort'].astype(str)\n",
                ")\n",
                "\n",
                "print(\"\\n--- Equalized Odds ---\")\n",
                "print(eo_df.to_string(index=False))\n",
                "print(f\"\\nTPR Gap (max - min): {tpr_gap:.3f}\")\n",
                "print(f\"FPR Gap (max - min): {fpr_gap:.3f}\")\n",
                "print(f\"{'✓ PASS' if tpr_gap <= 0.1 and fpr_gap <= 0.1 else '✗ REVIEW'}: TPR/FPR gap threshold is 0.10\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fairness Analysis by Region\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"FAIRNESS ANALYSIS - BY REGION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Demographic Parity\n",
                "dp_region_df, dp_region_ratio = compute_demographic_parity(\n",
                "    xgb_pred,\n",
                "    segment_test['primary_region']\n",
                ")\n",
                "\n",
                "print(\"\\n--- Demographic Parity ---\")\n",
                "print(dp_region_df.to_string(index=False))\n",
                "print(f\"\\nParity Ratio: {dp_region_ratio:.3f}\")\n",
                "print(f\"{'✓ PASS' if dp_region_ratio >= 0.8 else '✗ FAIL'}: Threshold is 0.80\")\n",
                "\n",
                "# Equalized Odds\n",
                "eo_region_df, tpr_region_gap, fpr_region_gap = compute_equalized_odds(\n",
                "    y_test_reset.values,\n",
                "    xgb_pred,\n",
                "    segment_test['primary_region']\n",
                ")\n",
                "\n",
                "print(\"\\n--- Equalized Odds ---\")\n",
                "print(eo_region_df.to_string(index=False))\n",
                "print(f\"\\nTPR Gap: {tpr_region_gap:.3f}\")\n",
                "print(f\"FPR Gap: {fpr_region_gap:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fairness Summary Visualization\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Demographic Parity by Cohort\n",
                "ax1 = axes[0]\n",
                "colors = ['coral' if r < 0.8 else 'seagreen' for r in [dp_ratio]]\n",
                "bars = ax1.bar(dp_df['group'], dp_df['selection_rate'], color='steelblue', edgecolor='black')\n",
                "ax1.axhline(y=dp_df['selection_rate'].mean(), color='red', linestyle='--', label='Mean Rate')\n",
                "ax1.set_ylabel('Selection Rate (Churn Prediction)')\n",
                "ax1.set_xlabel('User Cohort')\n",
                "ax1.set_title(f'Demographic Parity by Cohort\\n(Parity Ratio: {dp_ratio:.2f})', fontsize=12, fontweight='bold')\n",
                "ax1.legend()\n",
                "ax1.set_ylim(0, 1)\n",
                "\n",
                "# Equalized Odds by Cohort\n",
                "ax2 = axes[1]\n",
                "x = np.arange(len(eo_df))\n",
                "width = 0.35\n",
                "ax2.bar(x - width/2, eo_df['tpr'], width, label='TPR', color='steelblue')\n",
                "ax2.bar(x + width/2, eo_df['fpr'], width, label='FPR', color='coral')\n",
                "ax2.set_xticks(x)\n",
                "ax2.set_xticklabels(eo_df['group'])\n",
                "ax2.set_ylabel('Rate')\n",
                "ax2.set_xlabel('User Cohort')\n",
                "ax2.set_title(f'Equalized Odds by Cohort\\n(TPR Gap: {tpr_gap:.2f}, FPR Gap: {fpr_gap:.2f})', fontsize=12, fontweight='bold')\n",
                "ax2.legend()\n",
                "ax2.set_ylim(0, 1)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('fairness_analysis.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Export SHAP Data for API"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def export_shap_summary(shap_values, X, feature_names, output_path):\n",
                "    \"\"\"Export SHAP summary data as JSON for API consumption.\"\"\"\n",
                "    \n",
                "    # Compute mean absolute SHAP values\n",
                "    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
                "    \n",
                "    # Create summary\n",
                "    summary = {\n",
                "        'feature_importance': [\n",
                "            {'feature': f, 'mean_abs_shap': float(v)}\n",
                "            for f, v in sorted(zip(feature_names, mean_abs_shap), key=lambda x: -x[1])\n",
                "        ],\n",
                "        'expected_value': float(xgb_explainer.expected_value),\n",
                "        'n_samples': len(X),\n",
                "        'model_type': 'XGBoost',\n",
                "        'generated_at': datetime.now().isoformat()\n",
                "    }\n",
                "    \n",
                "    with open(output_path, 'w') as f:\n",
                "        json.dump(summary, f, indent=2)\n",
                "    \n",
                "    print(f\"SHAP summary exported to: {output_path}\")\n",
                "    return summary\n",
                "\n",
                "\n",
                "# Export global summary\n",
                "shap_summary = export_shap_summary(\n",
                "    xgb_shap_values,\n",
                "    X_test_reset,\n",
                "    feature_cols,\n",
                "    'shap_summary.json'\n",
                ")\n",
                "\n",
                "print(\"\\nTop 5 Features by SHAP Importance:\")\n",
                "for item in shap_summary['feature_importance'][:5]:\n",
                "    print(f\"  {item['feature']}: {item['mean_abs_shap']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def export_individual_explanation(idx, shap_values, X, feature_names, explainer, y_pred, y_prob):\n",
                "    \"\"\"Export individual prediction explanation.\"\"\"\n",
                "    \n",
                "    explanation = {\n",
                "        'prediction_id': f'pred_{idx}',\n",
                "        'predicted_class': int(y_pred[idx]),\n",
                "        'predicted_probability': float(y_prob[idx]),\n",
                "        'base_value': float(explainer.expected_value),\n",
                "        'shap_values': [\n",
                "            {'feature': f, 'value': float(X.iloc[idx][f]), 'shap': float(s)}\n",
                "            for f, s in zip(feature_names, shap_values[idx])\n",
                "        ],\n",
                "        'generated_at': datetime.now().isoformat()\n",
                "    }\n",
                "    \n",
                "    return explanation\n",
                "\n",
                "\n",
                "# Export sample explanations\n",
                "sample_explanations = []\n",
                "for i in range(min(10, len(X_test_reset))):\n",
                "    exp = export_individual_explanation(\n",
                "        i, xgb_shap_values, X_test_reset, feature_cols,\n",
                "        xgb_explainer, xgb_pred, xgb_prob\n",
                "    )\n",
                "    sample_explanations.append(exp)\n",
                "\n",
                "with open('sample_explanations.json', 'w') as f:\n",
                "    json.dump(sample_explanations, f, indent=2)\n",
                "\n",
                "print(f\"\\nExported {len(sample_explanations)} sample explanations to sample_explanations.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary\n",
                "\n",
                "### Key Results\n",
                "\n",
                "**SHAP Analysis:**\n",
                "- Computed SHAP values for XGBoost and LightGBM models\n",
                "- Generated summary plots showing feature importance\n",
                "- Created force plots for individual prediction explanations\n",
                "\n",
                "**Segment Performance:**\n",
                "- Analyzed model performance across regions, user cohorts, and channels\n",
                "- Identified potential performance disparities\n",
                "\n",
                "**Fairness Checks:**\n",
                "- Computed demographic parity (selection rate equality)\n",
                "- Computed equalized odds (TPR/FPR equality)\n",
                "- Generated fairness reports per sensitive attribute\n",
                "\n",
                "### Artifacts Generated\n",
                "- `shap_summary_xgboost.png` - XGBoost SHAP summary\n",
                "- `shap_summary_lightgbm.png` - LightGBM SHAP summary\n",
                "- `shap_bar_comparison.png` - Feature importance comparison\n",
                "- `shap_force_plot_single.png` - Single prediction force plot\n",
                "- `shap_dependence_total_revenue.png` - Dependence plot\n",
                "- `segment_performance_region.png` - Regional performance\n",
                "- `segment_performance_cohort.png` - Cohort performance\n",
                "- `fairness_analysis.png` - Fairness visualization\n",
                "- `shap_summary.json` - API-ready SHAP summary\n",
                "- `sample_explanations.json` - Sample individual explanations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\nNotebook execution complete at: {datetime.now().isoformat()}\")\n",
                "print(\"\\nGenerated artifacts:\")\n",
                "for f in OUTPUT_DIR.glob('*.png'):\n",
                "    print(f\"  - {f.name}\")\n",
                "for f in OUTPUT_DIR.glob('*.json'):\n",
                "    print(f\"  - {f.name}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}