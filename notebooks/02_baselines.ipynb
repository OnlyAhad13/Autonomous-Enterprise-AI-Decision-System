{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02 - Baseline Models for Churn Classification\n",
                "\n",
                "This notebook builds baseline classifiers for customer churn prediction:\n",
                "- Logistic Regression\n",
                "- XGBoost\n",
                "- LightGBM\n",
                "\n",
                "Features:\n",
                "- MLflow experiment tracking\n",
                "- Hyperparameter grid search\n",
                "- Automated model card generation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import json\n",
                "from pathlib import Path\n",
                "from datetime import datetime, timedelta\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# ML imports\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    roc_auc_score, confusion_matrix, classification_report,\n",
                "    roc_curve\n",
                ")\n",
                "import xgboost as xgb\n",
                "import lightgbm as lgb\n",
                "\n",
                "# MLflow\n",
                "import mlflow\n",
                "import mlflow.sklearn\n",
                "import mlflow.xgboost\n",
                "import mlflow.lightgbm\n",
                "\n",
                "# Visualization settings\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "%matplotlib inline\n",
                "\n",
                "print(\"Libraries loaded successfully!\")\n",
                "print(f\"MLflow version: {mlflow.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. MLflow Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure MLflow\n",
                "PROJECT_ROOT = Path(\".\").resolve().parent\n",
                "MLFLOW_TRACKING_URI = PROJECT_ROOT / \"mlruns\"\n",
                "EXPERIMENT_NAME = \"churn-classification-baselines\"\n",
                "\n",
                "mlflow.set_tracking_uri(str(MLFLOW_TRACKING_URI))\n",
                "mlflow.set_experiment(EXPERIMENT_NAME)\n",
                "\n",
                "print(f\"MLflow tracking URI: {MLFLOW_TRACKING_URI}\")\n",
                "print(f\"Experiment: {EXPERIMENT_NAME}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading & Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_events_data(delta_path: str = None, parquet_path: str = None) -> pd.DataFrame:\n",
                "    \"\"\"Load events data from Delta Lake or Parquet.\"\"\"\n",
                "    if delta_path:\n",
                "        try:\n",
                "            from deltalake import DeltaTable\n",
                "            dt = DeltaTable(delta_path)\n",
                "            df = dt.to_pandas()\n",
                "            print(f\"Loaded {len(df):,} rows from Delta: {delta_path}\")\n",
                "            return df\n",
                "        except Exception as e:\n",
                "            print(f\"Delta loading failed: {e}\")\n",
                "    \n",
                "    if parquet_path:\n",
                "        df = pd.read_parquet(parquet_path)\n",
                "        print(f\"Loaded {len(df):,} rows from Parquet: {parquet_path}\")\n",
                "        return df\n",
                "    \n",
                "    raise FileNotFoundError(\"No valid data source\")\n",
                "\n",
                "\n",
                "# Load data\n",
                "DELTA_PATH = PROJECT_ROOT / \"data\" / \"lake\" / \"delta\" / \"events\"\n",
                "PARQUET_PATH = PROJECT_ROOT / \"data\" / \"sample\" / \"events.parquet\"\n",
                "\n",
                "df = load_events_data(\n",
                "    delta_path=str(DELTA_PATH) if DELTA_PATH.exists() else None,\n",
                "    parquet_path=str(PARQUET_PATH)\n",
                ")\n",
                "\n",
                "print(f\"Dataset shape: {df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Parse metadata and timestamp\n",
                "if 'metadata' in df.columns and df['metadata'].dtype == 'object':\n",
                "    df['metadata'] = df['metadata'].apply(\n",
                "        lambda x: json.loads(x) if isinstance(x, str) else x\n",
                "    )\n",
                "\n",
                "df['channel'] = df['metadata'].apply(lambda x: x.get('channel') if isinstance(x, dict) else None)\n",
                "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
                "df['revenue'] = df['price'] * df['quantity']\n",
                "\n",
                "print(\"Preprocessing complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Churn Label Definition\n",
                "\n",
                "**Churn Definition**: A user is considered churned if they have no activity in the last 7 days of the dataset timeframe."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_churn_labels(df: pd.DataFrame, churn_window_days: int = 7) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Create churn labels based on user activity.\n",
                "    Handles both multi-transaction users and single-transaction synthetic data.\n",
                "    \n",
                "    For real data: Churned = no activity in the last N days of the dataset.\n",
                "    For synthetic single-transaction data: Creates simulated churn based on\n",
                "    transaction characteristics (recency + revenue percentile).\n",
                "    \"\"\"\n",
                "    max_date = df['timestamp'].max()\n",
                "    cutoff_date = max_date - timedelta(days=churn_window_days)\n",
                "    \n",
                "    # Split data: training period vs churn evaluation period\n",
                "    train_data = df[df['timestamp'] < cutoff_date].copy()\n",
                "    churn_period = df[df['timestamp'] >= cutoff_date].copy()\n",
                "    \n",
                "    # Users active in churn period = not churned\n",
                "    active_users = set(churn_period['user_id'].unique())\n",
                "    train_users = set(train_data['user_id'].unique())\n",
                "    \n",
                "    # Check if we have overlap (real multi-transaction data)\n",
                "    overlap = train_users & active_users\n",
                "    use_synthetic_labels = len(overlap) < 0.05 * len(train_users) if len(train_users) > 0 else True\n",
                "    \n",
                "    if use_synthetic_labels:\n",
                "        print(\"[INFO] Detected single-transaction-per-user data. Using synthetic churn labels.\")\n",
                "        # Use all data for feature engineering when each user has ~1 transaction\n",
                "        user_features = df.groupby('user_id').agg({\n",
                "            'id': 'count',\n",
                "            'revenue': ['sum', 'mean', 'std'],\n",
                "            'price': ['mean', 'max', 'min'],\n",
                "            'quantity': ['sum', 'mean'],\n",
                "            'timestamp': ['min', 'max'],\n",
                "            'channel': lambda x: x.mode().iloc[0] if len(x) > 0 else 'unknown'\n",
                "        }).reset_index()\n",
                "    else:\n",
                "        # Aggregate user features from training period only\n",
                "        user_features = train_data.groupby('user_id').agg({\n",
                "            'id': 'count',\n",
                "            'revenue': ['sum', 'mean', 'std'],\n",
                "            'price': ['mean', 'max', 'min'],\n",
                "            'quantity': ['sum', 'mean'],\n",
                "            'timestamp': ['min', 'max'],\n",
                "            'channel': lambda x: x.mode().iloc[0] if len(x) > 0 else 'unknown'\n",
                "        }).reset_index()\n",
                "    \n",
                "    # Flatten column names\n",
                "    user_features.columns = [\n",
                "        'user_id', 'transaction_count', 'total_revenue', 'avg_revenue', 'std_revenue',\n",
                "        'avg_price', 'max_price', 'min_price', 'total_quantity', 'avg_quantity',\n",
                "        'first_transaction', 'last_transaction', 'primary_channel'\n",
                "    ]\n",
                "    \n",
                "    # Calculate derived features (use max_date as reference for synthetic)\n",
                "    ref_date = max_date if use_synthetic_labels else cutoff_date\n",
                "    user_features['days_since_first'] = (\n",
                "        (ref_date - user_features['first_transaction']).dt.days\n",
                "    )\n",
                "    user_features['days_since_last'] = (\n",
                "        (ref_date - user_features['last_transaction']).dt.days\n",
                "    )\n",
                "    user_features['avg_days_between'] = (\n",
                "        user_features['days_since_first'] / user_features['transaction_count'].clip(lower=1)\n",
                "    )\n",
                "    \n",
                "    # Fill NaN in std_revenue (users with 1 transaction)\n",
                "    user_features['std_revenue'] = user_features['std_revenue'].fillna(0)\n",
                "    \n",
                "    # Create churn label\n",
                "    if use_synthetic_labels:\n",
                "        # Synthetic churn: Users with low revenue AND high recency are 'churned'\n",
                "        # This creates a ~30% churn rate based on bottom revenue quartile + recent inactivity\n",
                "        revenue_threshold = user_features['total_revenue'].quantile(0.35)\n",
                "        recency_threshold = user_features['days_since_last'].quantile(0.65)\n",
                "        \n",
                "        # Churn if: low revenue OR (medium revenue AND high recency)\n",
                "        user_features['churned'] = (\n",
                "            (user_features['total_revenue'] < revenue_threshold) |\n",
                "            ((user_features['total_revenue'] < user_features['total_revenue'].quantile(0.5)) &\n",
                "             (user_features['days_since_last'] > recency_threshold))\n",
                "        ).astype(int)\n",
                "        \n",
                "        print(f\"[INFO] Synthetic churn rate: {user_features['churned'].mean():.2%}\")\n",
                "    else:\n",
                "        user_features['churned'] = ~user_features['user_id'].isin(active_users)\n",
                "        user_features['churned'] = user_features['churned'].astype(int)\n",
                "    \n",
                "    # Drop timestamp columns (not needed for modeling)\n",
                "    user_features = user_features.drop(['first_transaction', 'last_transaction'], axis=1)\n",
                "    \n",
                "    return user_features\n",
                "\n",
                "\n",
                "# Create user-level dataset with churn labels\n",
                "CHURN_WINDOW = 7\n",
                "user_df = create_churn_labels(df, churn_window_days=CHURN_WINDOW)\n",
                "\n",
                "print(f\"\\nUser-level dataset: {user_df.shape}\")\n",
                "print(f\"\\nChurn distribution:\")\n",
                "print(user_df['churned'].value_counts(normalize=True).round(3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "user_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train/Validation Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define features and target\n",
                "feature_cols = [\n",
                "    'transaction_count', 'total_revenue', 'avg_revenue', 'std_revenue',\n",
                "    'avg_price', 'max_price', 'min_price', 'total_quantity', 'avg_quantity',\n",
                "    'days_since_first', 'days_since_last', 'avg_days_between'\n",
                "]\n",
                "\n",
                "X = user_df[feature_cols]\n",
                "y = user_df['churned']\n",
                "\n",
                "# Train/validation split (80/20)\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"Training set: {X_train.shape[0]:,} users\")\n",
                "print(f\"Validation set: {X_val.shape[0]:,} users\")\n",
                "print(f\"\\nTrain churn rate: {y_train.mean():.3f}\")\n",
                "print(f\"Val churn rate: {y_val.mean():.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale features for Logistic Regression\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_val_scaled = scaler.transform(X_val)\n",
                "\n",
                "print(\"Feature scaling complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_model(model, X_val, y_val, model_name: str):\n",
                "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
                "    y_pred = model.predict(X_val)\n",
                "    y_prob = model.predict_proba(X_val)[:, 1]\n",
                "    \n",
                "    metrics = {\n",
                "        'accuracy': accuracy_score(y_val, y_pred),\n",
                "        'precision': precision_score(y_val, y_pred),\n",
                "        'recall': recall_score(y_val, y_pred),\n",
                "        'f1': f1_score(y_val, y_pred),\n",
                "        'auc_roc': roc_auc_score(y_val, y_prob)\n",
                "    }\n",
                "    \n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"{model_name} Results\")\n",
                "    print(f\"{'='*50}\")\n",
                "    for metric, value in metrics.items():\n",
                "        print(f\"{metric.upper():<15} {value:.4f}\")\n",
                "    \n",
                "    return metrics, y_pred, y_prob\n",
                "\n",
                "\n",
                "def plot_confusion_matrix(y_true, y_pred, model_name: str, save_path: str = None):\n",
                "    \"\"\"Plot confusion matrix.\"\"\"\n",
                "    cm = confusion_matrix(y_true, y_pred)\n",
                "    plt.figure(figsize=(8, 6))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "                xticklabels=['Not Churned', 'Churned'],\n",
                "                yticklabels=['Not Churned', 'Churned'])\n",
                "    plt.title(f'Confusion Matrix - {model_name}', fontsize=12, fontweight='bold')\n",
                "    plt.ylabel('Actual')\n",
                "    plt.xlabel('Predicted')\n",
                "    plt.tight_layout()\n",
                "    if save_path:\n",
                "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "def plot_feature_importance(model, feature_names, model_name: str, save_path: str = None):\n",
                "    \"\"\"Plot feature importance.\"\"\"\n",
                "    if hasattr(model, 'feature_importances_'):\n",
                "        importance = model.feature_importances_\n",
                "    elif hasattr(model, 'coef_'):\n",
                "        importance = np.abs(model.coef_[0])\n",
                "    else:\n",
                "        return\n",
                "    \n",
                "    feat_imp = pd.DataFrame({\n",
                "        'feature': feature_names,\n",
                "        'importance': importance\n",
                "    }).sort_values('importance', ascending=True)\n",
                "    \n",
                "    plt.figure(figsize=(10, 6))\n",
                "    plt.barh(feat_imp['feature'], feat_imp['importance'], color='steelblue')\n",
                "    plt.title(f'Feature Importance - {model_name}', fontsize=12, fontweight='bold')\n",
                "    plt.xlabel('Importance')\n",
                "    plt.tight_layout()\n",
                "    if save_path:\n",
                "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Logistic Regression (Baseline)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with mlflow.start_run(run_name=\"LogisticRegression_baseline\"):\n",
                "    # Hyperparameter grid\n",
                "    param_grid = {\n",
                "        'C': [0.01, 0.1, 1.0, 10.0],\n",
                "        'penalty': ['l2'],\n",
                "        'solver': ['lbfgs'],\n",
                "        'max_iter': [1000]\n",
                "    }\n",
                "    \n",
                "    # Grid search\n",
                "    lr = LogisticRegression(random_state=42)\n",
                "    grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
                "    grid_search.fit(X_train_scaled, y_train)\n",
                "    \n",
                "    best_lr = grid_search.best_estimator_\n",
                "    \n",
                "    # Evaluate\n",
                "    lr_metrics, lr_pred, lr_prob = evaluate_model(best_lr, X_val_scaled, y_val, \"Logistic Regression\")\n",
                "    \n",
                "    # Log to MLflow\n",
                "    mlflow.log_params(grid_search.best_params_)\n",
                "    mlflow.log_metrics(lr_metrics)\n",
                "    mlflow.sklearn.log_model(best_lr, \"model\")\n",
                "    \n",
                "    # Save and log artifacts\n",
                "    plot_confusion_matrix(y_val, lr_pred, \"Logistic Regression\", \"lr_confusion_matrix.png\")\n",
                "    plot_feature_importance(best_lr, feature_cols, \"Logistic Regression\", \"lr_feature_importance.png\")\n",
                "    mlflow.log_artifact(\"lr_confusion_matrix.png\")\n",
                "    mlflow.log_artifact(\"lr_feature_importance.png\")\n",
                "    \n",
                "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
                "    lr_run_id = mlflow.active_run().info.run_id"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. XGBoost Classifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with mlflow.start_run(run_name=\"XGBoost_tuned\"):\n",
                "    # Hyperparameter grid\n",
                "    param_grid = {\n",
                "        'n_estimators': [100, 200],\n",
                "        'max_depth': [3, 5, 7],\n",
                "        'learning_rate': [0.01, 0.1],\n",
                "        'subsample': [0.8, 1.0],\n",
                "        'colsample_bytree': [0.8, 1.0]\n",
                "    }\n",
                "    \n",
                "    # Randomized search (faster than grid search)\n",
                "    xgb_clf = xgb.XGBClassifier(\n",
                "        random_state=42,\n",
                "        eval_metric='logloss',\n",
                "        use_label_encoder=False\n",
                "    )\n",
                "    \n",
                "    random_search = RandomizedSearchCV(\n",
                "        xgb_clf, param_grid, n_iter=20, cv=5,\n",
                "        scoring='roc_auc', n_jobs=-1, random_state=42\n",
                "    )\n",
                "    random_search.fit(X_train, y_train)\n",
                "    \n",
                "    best_xgb = random_search.best_estimator_\n",
                "    \n",
                "    # Evaluate\n",
                "    xgb_metrics, xgb_pred, xgb_prob = evaluate_model(best_xgb, X_val, y_val, \"XGBoost\")\n",
                "    \n",
                "    # Log to MLflow\n",
                "    mlflow.log_params(random_search.best_params_)\n",
                "    mlflow.log_metrics(xgb_metrics)\n",
                "    mlflow.xgboost.log_model(best_xgb, \"model\")\n",
                "    \n",
                "    # Save and log artifacts\n",
                "    plot_confusion_matrix(y_val, xgb_pred, \"XGBoost\", \"xgb_confusion_matrix.png\")\n",
                "    plot_feature_importance(best_xgb, feature_cols, \"XGBoost\", \"xgb_feature_importance.png\")\n",
                "    mlflow.log_artifact(\"xgb_confusion_matrix.png\")\n",
                "    mlflow.log_artifact(\"xgb_feature_importance.png\")\n",
                "    \n",
                "    print(f\"\\nBest parameters: {random_search.best_params_}\")\n",
                "    xgb_run_id = mlflow.active_run().info.run_id"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. LightGBM Classifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with mlflow.start_run(run_name=\"LightGBM_tuned\"):\n",
                "    # Hyperparameter grid\n",
                "    param_grid = {\n",
                "        'n_estimators': [100, 200],\n",
                "        'max_depth': [3, 5, 7, -1],\n",
                "        'learning_rate': [0.01, 0.05, 0.1],\n",
                "        'num_leaves': [31, 50, 100],\n",
                "        'subsample': [0.8, 1.0]\n",
                "    }\n",
                "    \n",
                "    lgb_clf = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
                "    \n",
                "    random_search = RandomizedSearchCV(\n",
                "        lgb_clf, param_grid, n_iter=20, cv=5,\n",
                "        scoring='roc_auc', n_jobs=-1, random_state=42\n",
                "    )\n",
                "    random_search.fit(X_train, y_train)\n",
                "    \n",
                "    best_lgb = random_search.best_estimator_\n",
                "    \n",
                "    # Evaluate\n",
                "    lgb_metrics, lgb_pred, lgb_prob = evaluate_model(best_lgb, X_val, y_val, \"LightGBM\")\n",
                "    \n",
                "    # Log to MLflow\n",
                "    mlflow.log_params(random_search.best_params_)\n",
                "    mlflow.log_metrics(lgb_metrics)\n",
                "    mlflow.lightgbm.log_model(best_lgb, \"model\")\n",
                "    \n",
                "    # Save and log artifacts\n",
                "    plot_confusion_matrix(y_val, lgb_pred, \"LightGBM\", \"lgb_confusion_matrix.png\")\n",
                "    plot_feature_importance(best_lgb, feature_cols, \"LightGBM\", \"lgb_feature_importance.png\")\n",
                "    mlflow.log_artifact(\"lgb_confusion_matrix.png\")\n",
                "    mlflow.log_artifact(\"lgb_feature_importance.png\")\n",
                "    \n",
                "    print(f\"\\nBest parameters: {random_search.best_params_}\")\n",
                "    lgb_run_id = mlflow.active_run().info.run_id"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare ROC curves\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "\n",
                "models = [\n",
                "    ('Logistic Regression', lr_prob, 'steelblue'),\n",
                "    ('XGBoost', xgb_prob, 'coral'),\n",
                "    ('LightGBM', lgb_prob, 'seagreen')\n",
                "]\n",
                "\n",
                "for name, probs, color in models:\n",
                "    fpr, tpr, _ = roc_curve(y_val, probs)\n",
                "    auc = roc_auc_score(y_val, probs)\n",
                "    ax.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', color=color, linewidth=2)\n",
                "\n",
                "ax.plot([0, 1], [0, 1], 'k--', label='Random Guess', alpha=0.5)\n",
                "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
                "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
                "ax.set_title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n",
                "ax.legend(loc='lower right', fontsize=10)\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('roc_comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary table\n",
                "comparison_df = pd.DataFrame({\n",
                "    'Model': ['Logistic Regression', 'XGBoost', 'LightGBM'],\n",
                "    'Accuracy': [lr_metrics['accuracy'], xgb_metrics['accuracy'], lgb_metrics['accuracy']],\n",
                "    'Precision': [lr_metrics['precision'], xgb_metrics['precision'], lgb_metrics['precision']],\n",
                "    'Recall': [lr_metrics['recall'], xgb_metrics['recall'], lgb_metrics['recall']],\n",
                "    'F1 Score': [lr_metrics['f1'], xgb_metrics['f1'], lgb_metrics['f1']],\n",
                "    'AUC-ROC': [lr_metrics['auc_roc'], xgb_metrics['auc_roc'], lgb_metrics['auc_roc']]\n",
                "}).round(4)\n",
                "\n",
                "print(\"\\nModel Comparison Summary:\")\n",
                "print(\"=\" * 80)\n",
                "print(comparison_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Generate Model Card"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_model_card(\n",
                "    model_name: str,\n",
                "    metrics: dict,\n",
                "    best_params: dict,\n",
                "    feature_names: list,\n",
                "    train_size: int,\n",
                "    val_size: int,\n",
                "    churn_rate: float,\n",
                "    output_path: str\n",
                "):\n",
                "    \"\"\"Generate a model card markdown file.\"\"\"\n",
                "    \n",
                "    template = f\"\"\"# Model Card: {model_name}\n",
                "\n",
                "## Model Details\n",
                "\n",
                "| Field | Value |\n",
                "|-------|-------|\n",
                "| Model Name | {model_name} |\n",
                "| Version | 1.0.0 |\n",
                "| Date | {datetime.now().strftime('%Y-%m-%d')} |\n",
                "| Task | Binary Classification (Churn Prediction) |\n",
                "| Framework | scikit-learn / XGBoost / LightGBM |\n",
                "\n",
                "## Dataset Statistics\n",
                "\n",
                "| Metric | Value |\n",
                "|--------|-------|\n",
                "| Training Samples | {train_size:,} |\n",
                "| Validation Samples | {val_size:,} |\n",
                "| Churn Rate | {churn_rate:.2%} |\n",
                "| Features | {len(feature_names)} |\n",
                "\n",
                "### Features Used\n",
                "\n",
                "{chr(10).join(['- ' + f for f in feature_names])}\n",
                "\n",
                "## Hyperparameters\n",
                "\n",
                "```json\n",
                "{json.dumps(best_params, indent=2)}\n",
                "```\n",
                "\n",
                "## Performance Metrics\n",
                "\n",
                "| Metric | Score |\n",
                "|--------|-------|\n",
                "| Accuracy | {metrics['accuracy']:.4f} |\n",
                "| Precision | {metrics['precision']:.4f} |\n",
                "| Recall | {metrics['recall']:.4f} |\n",
                "| F1 Score | {metrics['f1']:.4f} |\n",
                "| AUC-ROC | {metrics['auc_roc']:.4f} |\n",
                "\n",
                "## Intended Use\n",
                "\n",
                "This model is intended to predict customer churn based on transaction behavior.\n",
                "Use cases include:\n",
                "- Identifying at-risk customers for retention campaigns\n",
                "- Prioritizing customer success outreach\n",
                "- Informing loyalty program targeting\n",
                "\n",
                "## Limitations\n",
                "\n",
                "- Trained on synthetic business events data\n",
                "- Churn definition (7-day inactivity) may not align with all business contexts\n",
                "- Performance may degrade with distribution shift over time\n",
                "- Should be retrained periodically with fresh data\n",
                "\n",
                "## Ethical Considerations\n",
                "\n",
                "- Model predictions should not be the sole basis for consequential decisions\n",
                "- Human review recommended for high-impact interventions\n",
                "- Ensure fair treatment across customer segments\n",
                "\n",
                "## MLflow Tracking\n",
                "\n",
                "Experiment: `churn-classification-baselines`\n",
                "\n",
                "---\n",
                "*Generated automatically by `02_baselines.ipynb`*\n",
                "\"\"\"\n",
                "    \n",
                "    with open(output_path, 'w') as f:\n",
                "        f.write(template)\n",
                "    \n",
                "    print(f\"Model card saved to: {output_path}\")\n",
                "\n",
                "\n",
                "# Find best model\n",
                "best_model_metrics = max(\n",
                "    [('Logistic Regression', lr_metrics, grid_search.best_params_),\n",
                "     ('XGBoost', xgb_metrics, random_search.best_params_),\n",
                "     ('LightGBM', lgb_metrics, random_search.best_params_)],\n",
                "    key=lambda x: x[1]['auc_roc']\n",
                ")\n",
                "\n",
                "# Generate model card for best model\n",
                "generate_model_card(\n",
                "    model_name=best_model_metrics[0],\n",
                "    metrics=best_model_metrics[1],\n",
                "    best_params=best_model_metrics[2],\n",
                "    feature_names=feature_cols,\n",
                "    train_size=len(X_train),\n",
                "    val_size=len(X_val),\n",
                "    churn_rate=y.mean(),\n",
                "    output_path='model_card.md'\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Summary\n",
                "\n",
                "### Key Results\n",
                "- Trained 3 baseline models: Logistic Regression, XGBoost, and LightGBM\n",
                "- All experiments logged to MLflow for reproducibility\n",
                "- Generated automated model card for documentation\n",
                "\n",
                "### Next Steps\n",
                "- Feature engineering improvements (e.g., time-based features, RFM segmentation)\n",
                "- Ensemble methods or stacking\n",
                "- Threshold optimization for business objectives\n",
                "- Production deployment via FastAPI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\nNotebook execution complete at: {datetime.now().isoformat()}\")\n",
                "print(f\"\\nMLflow runs:\")\n",
                "print(f\"  - Logistic Regression: {lr_run_id}\")\n",
                "print(f\"  - XGBoost: {xgb_run_id}\")\n",
                "print(f\"  - LightGBM: {lgb_run_id}\")\n",
                "print(f\"\\nView experiments with: mlflow ui --backend-store-uri {MLFLOW_TRACKING_URI}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}