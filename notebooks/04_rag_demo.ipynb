{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RAG (Retrieval-Augmented Generation) Demo\n",
                "\n",
                "This notebook demonstrates how to use the RAG pipeline to:\n",
                "1. Ingest documents and create vector embeddings\n",
                "2. Query the index to retrieve relevant documents\n",
                "3. Use an LLM to generate answers based on retrieved context"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# Add project root to path\n",
                "PROJECT_ROOT = Path.cwd().parent\n",
                "sys.path.insert(0, str(PROJECT_ROOT))\n",
                "\n",
                "print(f\"Project root: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages if needed\n",
                "# !pip install sentence-transformers faiss-cpu openai"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Document Ingestion\n",
                "\n",
                "Ingest documents from `data/docs/` and create vector index."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from services.rag.ingest_vectors import VectorIngester\n",
                "\n",
                "# Initialize ingester\n",
                "ingester = VectorIngester(\n",
                "    docs_dir=PROJECT_ROOT / \"data\" / \"docs\",\n",
                "    index_dir=PROJECT_ROOT / \"data\" / \"vector_index\",\n",
                "    model_name=\"all-MiniLM-L6-v2\",\n",
                "    chunk_size=512,\n",
                "    chunk_overlap=50,\n",
                ")\n",
                "\n",
                "# Run ingestion\n",
                "num_chunks = ingester.ingest()\n",
                "print(f\"\\n‚úì Ingested {num_chunks} document chunks\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Query the Index\n",
                "\n",
                "Retrieve relevant documents for a query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from services.rag.retriever import retrieve\n",
                "\n",
                "# Test query\n",
                "query = \"What is machine learning?\"\n",
                "results = retrieve(query, top_k=3)\n",
                "\n",
                "print(f\"Query: {query}\\n\")\n",
                "print(\"=\" * 60)\n",
                "for i, result in enumerate(results, 1):\n",
                "    print(f\"\\n[{i}] Score: {result['score']:.4f}\")\n",
                "    print(f\"    Source: {result['source']}\")\n",
                "    print(f\"    Content: {result['content'][:200]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Another query\n",
                "query = \"How does RAG work?\"\n",
                "results = retrieve(query, top_k=3)\n",
                "\n",
                "print(f\"Query: {query}\\n\")\n",
                "print(\"=\" * 60)\n",
                "for i, result in enumerate(results, 1):\n",
                "    print(f\"\\n[{i}] Score: {result['score']:.4f}\")\n",
                "    print(f\"    Source: {result['source']}\")\n",
                "    print(f\"    Content: {result['content'][:200]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. RAG with LLM\n",
                "\n",
                "Combine retrieval with an LLM to generate grounded answers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_rag_prompt(query: str, context_docs: list, max_context_length: int = 2000) -> str:\n",
                "    \"\"\"\n",
                "    Build a RAG prompt with retrieved context.\n",
                "    \n",
                "    Args:\n",
                "        query: User's question\n",
                "        context_docs: Retrieved documents\n",
                "        max_context_length: Maximum context characters\n",
                "        \n",
                "    Returns:\n",
                "        Formatted prompt string\n",
                "    \"\"\"\n",
                "    # Build context section\n",
                "    context_parts = []\n",
                "    total_length = 0\n",
                "    \n",
                "    for doc in context_docs:\n",
                "        content = doc['content']\n",
                "        if total_length + len(content) > max_context_length:\n",
                "            content = content[:max_context_length - total_length]\n",
                "        context_parts.append(f\"[Source: {doc['source']}]\\n{content}\")\n",
                "        total_length += len(content)\n",
                "        if total_length >= max_context_length:\n",
                "            break\n",
                "    \n",
                "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
                "    \n",
                "    prompt = f\"\"\"You are a helpful assistant. Answer the user's question based on the provided context.\n",
                "If the context doesn't contain relevant information, say so.\n",
                "\n",
                "CONTEXT:\n",
                "{context}\n",
                "\n",
                "USER QUESTION: {query}\n",
                "\n",
                "ANSWER:\"\"\"\n",
                "    \n",
                "    return prompt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Option A: Using OpenAI API"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "def rag_query_openai(query: str, top_k: int = 3) -> str:\n",
                "    \"\"\"\n",
                "    RAG query using OpenAI API.\n",
                "    \n",
                "    Requires: OPENAI_API_KEY environment variable\n",
                "    \"\"\"\n",
                "    try:\n",
                "        from openai import OpenAI\n",
                "    except ImportError:\n",
                "        return \"OpenAI package not installed. Run: pip install openai\"\n",
                "    \n",
                "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
                "    if not api_key:\n",
                "        return \"OPENAI_API_KEY not set. Set it with: export OPENAI_API_KEY=your_key\"\n",
                "    \n",
                "    # Retrieve documents\n",
                "    results = retrieve(query, top_k=top_k)\n",
                "    \n",
                "    # Build prompt\n",
                "    prompt = build_rag_prompt(query, results)\n",
                "    \n",
                "    # Query LLM\n",
                "    client = OpenAI(api_key=api_key)\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[\n",
                "            {\"role\": \"user\", \"content\": prompt}\n",
                "        ],\n",
                "        max_tokens=500,\n",
                "        temperature=0.7,\n",
                "    )\n",
                "    \n",
                "    return response.choices[0].message.content\n",
                "\n",
                "# Example (uncomment to run if you have an API key)\n",
                "# answer = rag_query_openai(\"What are the types of machine learning?\")\n",
                "# print(answer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Option B: Using Local LLM (Ollama)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "\n",
                "def rag_query_ollama(\n",
                "    query: str, \n",
                "    top_k: int = 3, \n",
                "    model: str = \"llama2\",\n",
                "    base_url: str = \"http://localhost:11434\"\n",
                ") -> str:\n",
                "    \"\"\"\n",
                "    RAG query using Ollama local LLM.\n",
                "    \n",
                "    Requires: Ollama running locally with a model pulled\n",
                "    \"\"\"\n",
                "    # Retrieve documents\n",
                "    results = retrieve(query, top_k=top_k)\n",
                "    \n",
                "    # Build prompt\n",
                "    prompt = build_rag_prompt(query, results)\n",
                "    \n",
                "    try:\n",
                "        response = requests.post(\n",
                "            f\"{base_url}/api/generate\",\n",
                "            json={\n",
                "                \"model\": model,\n",
                "                \"prompt\": prompt,\n",
                "                \"stream\": False,\n",
                "            },\n",
                "            timeout=60,\n",
                "        )\n",
                "        response.raise_for_status()\n",
                "        return response.json().get(\"response\", \"No response\")\n",
                "    except requests.exceptions.ConnectionError:\n",
                "        return \"Ollama not running. Start it with: ollama serve\"\n",
                "    except Exception as e:\n",
                "        return f\"Error: {e}\"\n",
                "\n",
                "# Example (uncomment to run if you have Ollama)\n",
                "# answer = rag_query_ollama(\"What are the components of the enterprise AI system?\")\n",
                "# print(answer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Option C: Simple Template-Based Response (No LLM)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def rag_query_simple(query: str, top_k: int = 3) -> str:\n",
                "    \"\"\"\n",
                "    Simple RAG query without LLM - just formats retrieved results.\n",
                "    Useful for testing the retrieval pipeline.\n",
                "    \"\"\"\n",
                "    results = retrieve(query, top_k=top_k)\n",
                "    \n",
                "    if not results:\n",
                "        return f\"No relevant documents found for: {query}\"\n",
                "    \n",
                "    response = f\"Based on the knowledge base, here's what I found about '{query}':\\n\\n\"\n",
                "    \n",
                "    for i, doc in enumerate(results, 1):\n",
                "        response += f\"**Finding {i}** (Score: {doc['score']:.2f}, Source: {doc['source']})\\n\"\n",
                "        response += f\"{doc['content'][:300]}...\\n\\n\"\n",
                "    \n",
                "    return response\n",
                "\n",
                "# Test\n",
                "answer = rag_query_simple(\"What is RAG and how does it work?\")\n",
                "print(answer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Interactive RAG Demo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Interactive query function\n",
                "def ask(question: str, use_llm: str = \"simple\") -> None:\n",
                "    \"\"\"\n",
                "    Interactive query interface.\n",
                "    \n",
                "    Args:\n",
                "        question: Your question\n",
                "        use_llm: \"simple\", \"openai\", or \"ollama\"\n",
                "    \"\"\"\n",
                "    print(f\"\\nüîç Question: {question}\\n\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    if use_llm == \"openai\":\n",
                "        answer = rag_query_openai(question)\n",
                "    elif use_llm == \"ollama\":\n",
                "        answer = rag_query_ollama(question)\n",
                "    else:\n",
                "        answer = rag_query_simple(question)\n",
                "    \n",
                "    print(f\"\\nüí° Answer:\\n{answer}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try some questions!\n",
                "ask(\"What are the key algorithms in machine learning?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ask(\"How does the feature store work in this system?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ask(\"What are the benefits of using RAG?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. API Usage Example\n",
                "\n",
                "Query the retriever API (requires running the server first)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start the API server in a terminal:\n",
                "# uvicorn services.rag.retriever:app --port 8002 --reload\n",
                "\n",
                "import requests\n",
                "\n",
                "def query_api(query: str, top_k: int = 3, base_url: str = \"http://localhost:8002\"):\n",
                "    \"\"\"Query the retriever API.\"\"\"\n",
                "    try:\n",
                "        response = requests.post(\n",
                "            f\"{base_url}/retrieve\",\n",
                "            json={\"query\": query, \"top_k\": top_k},\n",
                "            timeout=10,\n",
                "        )\n",
                "        response.raise_for_status()\n",
                "        return response.json()\n",
                "    except requests.exceptions.ConnectionError:\n",
                "        return {\"error\": \"API not running. Start it with: uvicorn services.rag.retriever:app --port 8002\"}\n",
                "\n",
                "# Example (uncomment when API is running)\n",
                "# result = query_api(\"What is supervised learning?\")\n",
                "# print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook demonstrated:\n",
                "\n",
                "1. **Document Ingestion**: Loading docs, chunking, and creating embeddings\n",
                "2. **Vector Search**: Finding relevant documents using semantic similarity\n",
                "3. **RAG Pipeline**: Combining retrieval with LLM for grounded responses\n",
                "4. **API Integration**: Using the FastAPI retriever endpoint\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "- Add more documents to `data/docs/`\n",
                "- Fine-tune chunk size and overlap for your content\n",
                "- Experiment with different embedding models\n",
                "- Connect to a production LLM for better responses"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}